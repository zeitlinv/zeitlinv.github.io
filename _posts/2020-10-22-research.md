---
layout: post
title: Meta-Analysis of Research
---

### Overview and Data Collection

The Pew Research Center study, entitled ["Teens, Social Media & Technology 2018"](https://www.pewresearch.org/internet/2018/05/31/teens-social-media-technology-2018/) covers usage of technology in American teenagers, specifically focusing on social media-related technology. Many of the statistics included in the report aim to show the overall percentage of Americans using a particular technology or the differing opinions and tech usage of various demographics. The data used was collected through surveys offered to randomly selected American households, with a total of 1058 surveyed parents (who have children aged 13-17) and 743 surveyed teens. The survey was offered through [AmeriSpeak](https://www.amerispeak.org/), a research panel operated by NORC (National Opinion Research Center) at the University of Chicago. The panel claims to have a high response rate, in addition to representing almost the entire U.S. population (97%) through probability-based sample selection. The households that are parts of the panel are recruited thorugh means that can reach a wide variety of people such as U.S. mail, the telephone, and in-person interviews. Based on this information, I believe that the researchers adequately represent a wide breadth of the population with their study. In the data itself, one of the attributes that stood out to me was Weight, which varied drastically between respondees. These weights were supposedly first assigned by the research panel, based on particular demographics and the totals of those groups in the population as a whole. The researchers noted in the report that they actually adjusted these weights to account for those who were selected but chose not to respond to the survey. In the data tables, I saw that 216 parents and 332 teenagers did not complete the survey. At first, I wondered if this type of adjustment may skew the data, but I realized that having some respondees' answers overrepresented may be better than the alternative of having the bias that comes as a result of certain groups being potentially less likely to respond than others. The margin for sampling error (which is due to a sample being unrepresentative) was around 5% for both the teens and parents. I believe that this error is acceptable considering the amount and variety of people the study aimed to include. Also, sub groups of the sample often had greater margin for error, which I think is a result of the differing weights for participants.

### The Data

In the data table, the columns included the respondent's ID number, the language of the survey, each of the questions, participant-based categories such as gender and income, information on how the survey was taken, and the respondent's weight. I also looked at the topline report, which contained some information on the questions asked in the survey, how they were presented to a participant, and the percentages of people by whom each answer choice was chosen. I read through many of the questions and found that they were mostly worded clearly, and I noticed that the researchers paid close attention to detail when designing the survey. For instance, the researchers randomized the order in which answer choices showed up on the survey, preventing some choices from being more likely options due to their placement. Because some people could be more likely to interpret a question a certain way based on their education level, I still think there is some bias in the questions, but this type of issue is probably unavoidable in such a large scale study.

### The Research Report

In the report itself, I do not think that the researchers attempted to show false or exaggerated findings. The purpose of the report was mainly to prove that teenagers are spending most of their time online, and it worked to highlight some trends that became apparent through the researchers' data. Most of the points included were backed with percentages from the data, which helps to prevent people from assuming that certain findings exist to a greater extent than they really do. Also, I saw that any opinion-based information, such as whether social media has a negative effect on teens' lives, would include responses by advocates for both sides of the argument. One aspect of the paper that I thought was slightly misleading was the use of broad statistics, for instance: "Fully 95% of teens have access to a smartphone, and 45% say they are online 'almost constantly.'" This initially led me to assume that a much larger sample size had been used, since the statistics seemed to be applying to "all teens." If the researchers intend on using this language, I think they should mention at least an estimate of their sample size towards the top of their article. Generally, because almost every finding included was backed by carefully collected data, I do not think "publish or perish" motivated the researchers, and the Pew Research Center is a nonprofit organization. First, I found that the collection of data was meticulously done to represent the American teenage population as accurately as possible, as opposed to rushed or manipulated. I did not find any places in the report where the researchers seemed to be attempting to deceive readers by leaving out data or making a finding artificially more important. In addition, the researchers included an in-depth section on how they collected their data and used AmeriSpeak, and they made downloading the dataset very accessible.