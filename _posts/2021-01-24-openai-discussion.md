---
layout: post
title: OpenAI Discussion
---

#### What are some of the ambiguities regarding artificial general intelligence (AGI)?

Based on past instances of AI, problems such as bias clearly exist in algorithms, so the implementation of AGI may or may not be dangerous, especially when only certain organizations have access to it. It is still unclear what AGI will actually be like, considering types of intelligence that may exist outside human intelligence. Although companies focusing on pursuing AGI have received great attention from investors, in the past, the field has seemed more promising than it ended up being. With the many ways in which AGI could surpass the capabilities of the human mind, it still currently has great potential to work on problems common throughout the world, assuming its developers use the technology responsibly. 

#### What was OpenAI’s initial mission and philosophy?

OpenAI is a nonprofit, and it hoped to develop AGI technology safely and provide the value of the technology across the world, not to itself and its shareholders. They believed that AGI should be for the benefit of humanity, and they were willing to collaborate with another organization if they were closer to achieving AGI. The idea is that due to being an “open” nonprofit organization, they would be more committed to the goal of safe and accessible AGI for everyone than organizations with commercial interests. However, a closer investigation of OpenAI and its employees proved that due to secrecy, OpenAI may be more committed to protecting the image of that philosophy than actually upholding it.
	
#### What strategy does OpenAI seem to be adopting for achieving AGI?

They originally had more of an informal structure, merely working towards a broad goal that did not provide them with much of a sense of direction, but they soon had to gravitate towards a more structured model, which led them to find out that more resources were necessary for actually developing AGI and competing with companies such as DeepMind. This change led to the creation of the charter, a document that outlined their central philosophy but also included the importance of resources. Under this more organized structure, they have released various types of research and models so far, with one of the first well-known pieces of research being GPT-2, a deep learning-based model. At the present, many of OpenAI’s research and models have involved adapting and improving already developed AI techniques, rather than starting with completely new methods, a strategy that may be necessary to attain AGI. They use multiple teams to simultaneously work on different technologies that may each be the ideal path, and additional teams work on ensuring the safety and explainability of the AI processes. Their next algorithm, which was said to be trained on many types of data using immense resources, is thought by OpenAI to create the best strategy for working towards AGI.

#### Elaborate on a few of the current criticisms of OpenAI.

Upon realizing that more resources would be necessary for achieving AGI, OpenAI somewhat left its nonprofit status by placing a 100-fold limit on investors’ returns. Due to the unlikeliness of reaching such a return, the limit did not seem to limit much at all, making it seem as if OpenAI was concentrating the benefits of AI as its mission had promised not to. In terms of the research papers it has published, many have felt that OpenAI have exaggerated how advanced their models were to gain more attention and publicity. This trend began with GPT-2, where OpenAI claimed that the model was too dangerous to make public. Contradicting its promised “openness” and the qualities associated with nonprofits, models such as this one were often kept mostly secret in reaching AGI before others, leading some to fear that OpenAI may be planning to license the technology later on.

#### What do you personally think AGI should be capable of? In other words, what do you imagine successfully-achieved AGI to be?

I believe that the most important part of a successful AGI is that it is distributed across the world and not concentrated among a few groups of people. The power of AGI could be dangerous when only some have access to it, or if the technology itself was not developed with the risks and biases of AI algorithms in mind. I think that AGI could have many applications in solving global problems, as it would be able to at once consider many more factors and understand a far greater amount of research than any human. It could also optimize the solution it comes up with by predicting many of the issues that may arise from certain thought processes. I am also interested in the potential of natural language processing, and the ability of an AI to communicate with and understand the perspectives of humans. In general, I would like to see an AI that can understand many fields at once, which is one of the ways AGI may be able to improve upon the intelligence of humans and current AI to the greatest extent.